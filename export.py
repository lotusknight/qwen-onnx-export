import os
import torch
import shutil
from pathlib import Path
from transformers import AutoTokenizer
from optimum.onnxruntime import ORTModelForCausalLM

# --- ç¯å¢ƒå˜é‡é…ç½® ---
os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"

def get_model_weights(model_id, cache_root="./model_weights"):
    """
    è·å–æ¨¡å‹æƒé‡ï¼Œè¿”å›è·¯å¾„
    """
    safe_name = model_id.replace("/", "--")
    local_path = Path(cache_root) / safe_name
    
    # æ£€æŸ¥æœ¬åœ°æ˜¯å¦å·²å­˜åœ¨å®Œæ•´æ¨¡å‹
    if local_path.exists() and (local_path / "config.json").exists():
        print(f"âœ… å‘ç°æœ¬åœ°ç¼“å­˜: {local_path.absolute()}")
        return str(local_path), True # è¿”å›è·¯å¾„åŠâ€œæ˜¯å¦åŸæœ¬å°±åœ¨æœ¬åœ°â€
    
    print(f"ğŸš€ æœ¬åœ°æœªå‘ç°æ¨¡å‹ï¼Œå¼€å§‹ä¸‹è½½...")
    local_path.mkdir(parents=True, exist_ok=True)
    
    # ä¼˜å…ˆå°è¯• HF é•œåƒ
    try:
        from huggingface_hub import snapshot_download
        path = snapshot_download(
            repo_id=model_id,
            local_dir=local_path,
            local_dir_use_symlinks=False,
            resume_download=True,
            ignore_patterns=["*.msgpack", "*.h5", "*.tflite"]
        )
        return path, False
    except Exception as e:
        print(f"âš ï¸ HF ä¸‹è½½å¤±è´¥ï¼Œå°è¯• ModelScope: {e}")
        from modelscope import snapshot_download as ms_snapshot
        path = ms_snapshot(model_id=model_id, local_dir=str(local_path))
        return path, False

def export_to_onnx(model_id, output_dir, keep_weights=True):
    """
    å¯¼å‡ºä¸»å‡½æ•°
    :param model_id: æ¨¡å‹ ID
    :param output_dir: ONNX è¾“å‡ºè·¯å¾„
    :param keep_weights: æ˜¯å¦ä¿ç•™åŸå§‹æƒé‡ (é»˜è®¤ True)
    """
    # 1. å‡†å¤‡æƒé‡
    weights_path, already_existed = get_model_weights(model_id)
    
    try:
        # 2. æ‰§è¡Œå¯¼å‡º
        print(f"\nğŸ“¦ å¼€å§‹å¯¼å‡º ONNX è‡³: {output_dir}")
        use_fp16 = torch.cuda.is_available()
        
        model = ORTModelForCausalLM.from_pretrained(
            weights_path,
            export=True,
            task="text-generation-with-past",
            trust_remote_code=True,
            torch_dtype=torch.float16 if use_fp16 else torch.float32
        )

        model.save_pretrained(output_dir)
        tokenizer = AutoTokenizer.from_pretrained(weights_path)
        tokenizer.save_pretrained(output_dir)

        # 3. OnnxSlim ä¼˜åŒ–
        print(f"\nğŸª„ æ­£åœ¨è¿è¡Œ OnnxSlim ä¼˜åŒ–...")
        import onnx, onnxslim
        for p in Path(output_dir).glob("*.onnx"):
            print(f"   ä¼˜åŒ–ä¸­: {p.name}")
            slim_model = onnxslim.slim(onnx.load(str(p)))
            onnx.save(slim_model, str(p))
            
        print("\nâœ¨ ONNX å¯¼å‡ºä¸ä¼˜åŒ–æˆåŠŸå®Œæˆï¼")

    finally:
        # 4. æ¸…ç†é€»è¾‘
        # å¦‚æœ keep_weights ä¸º Falseï¼Œä¸”æ¨¡å‹æ˜¯æœ¬æ¬¡è„šæœ¬åˆšä¸‹è½½çš„ï¼Œåˆ™æ¸…ç†
        if not keep_weights:
            print(f"\nğŸ—‘ï¸ å‚æ•° keep_weights=Falseï¼Œæ­£åœ¨æ¸…ç†åŸå§‹æƒé‡ç›®å½•: {weights_path}")
            try:
                # ä½¿ç”¨ shutil.rmtree åˆ é™¤æ•´ä¸ªæ–‡ä»¶å¤¹
                shutil.rmtree(weights_path)
                print("âœ… åŸå§‹æƒé‡å·²æ¸…ç†ã€‚")
            except Exception as e:
                print(f"âŒ æ¸…ç†å¤±è´¥: {e}")
        else:
            print(f"\nğŸ’¾ åŸå§‹æƒé‡ä¿ç•™åœ¨: {weights_path}")

if __name__ == "__main__":
    # --- ç”¨æˆ·é…ç½®åŒº ---
    TARGET_MODEL = "Qwen/Qwen2.5-0.5B-Instruct"
    EXPORT_PATH = "./qwen25_05b_onnx"
    
    # è®¾ç½®ä¸º True åˆ™ä¿ç•™ä¸‹è½½çš„ 1GB+ åŸå§‹æƒé‡
    # è®¾ç½®ä¸º False åˆ™åœ¨ç”Ÿæˆ ONNX ååˆ é™¤åŸå§‹æƒé‡ï¼ŒèŠ‚çœç©ºé—´
    KEEP_ORIGINAL = True 

    export_to_onnx(TARGET_MODEL, EXPORT_PATH, keep_weights=KEEP_ORIGINAL)

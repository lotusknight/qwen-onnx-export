"""å°† Qwen ç±»å› æœè¯­è¨€æ¨¡å‹å¯¼å‡ºä¸º ONNXï¼Œå¯é€‰ OnnxSlim ä¼˜åŒ–ã€‚"""

import argparse
import os
import shutil
import tempfile
from pathlib import Path

import onnx
import onnxslim
from transformers import AutoTokenizer
from optimum.onnxruntime import ORTModelForCausalLM


def get_model_weights(
    model_id: str, cache_root: str = "./model_weights"
) -> tuple[str, bool]:
    """è·å–æ¨¡å‹æƒé‡ï¼šæœ‰æœ¬åœ°ç¼“å­˜åˆ™ç›´æ¥ç”¨ï¼Œå¦åˆ™ç”¨ ModelScope ä¸‹è½½ã€‚è¿”å› (è·¯å¾„, æ˜¯å¦åŸæœ¬å°±å­˜åœ¨)ã€‚"""
    safe_name = model_id.replace("/", "--")
    local_path = Path(cache_root) / safe_name

    # å·²æœ‰å®Œæ•´ç¼“å­˜åˆ™ç›´æ¥è¿”å›
    if local_path.exists() and (local_path / "config.json").exists():
        print(f"âœ… å‘ç°æœ¬åœ°ç¼“å­˜: {local_path.absolute()}")
        return str(local_path), True

    print("ğŸš€ æœ¬åœ°æœªå‘ç°æ¨¡å‹ï¼Œä½¿ç”¨ ModelScope ä¸‹è½½...")
    local_path.mkdir(parents=True, exist_ok=True)
    from modelscope import snapshot_download as ms_snapshot

    path = ms_snapshot(model_id=model_id, local_dir=str(local_path))
    return path, False


def _provider_for_device(device: str) -> str:
    """æ ¹æ® device å­—ç¬¦ä¸²è¿”å› ONNX Runtime çš„ providerã€‚"""
    if device.lower() == "cuda":
        return "CUDAExecutionProvider"
    return "CPUExecutionProvider"


def export_to_onnx(
    model_id: str,
    output_dir: str,
    *,
    device: str = "cpu",
    dtype: str = "fp32",
    keep_weights: bool = True,
    use_onnxslim: bool = False,
) -> None:
    """
    å¯¼å‡ºä¸»æµç¨‹ï¼šæ‹‰å–/åŠ è½½æƒé‡ â†’ å¯¼å‡º ONNX â†’ å¯é€‰ OnnxSlim â†’ å¯é€‰åˆ é™¤æƒé‡ã€‚
    device: cpu / cudaï¼›dtype: fp32 / fp16ï¼›keep_weights ä¸º False ä¸”å¯¼å‡ºæˆåŠŸæ—¶åˆ é™¤æƒé‡ç›®å½•ã€‚
    """
    weights_path, _ = get_model_weights(model_id)
    export_succeeded = False
    provider = _provider_for_device(device)

    try:
        # 1. å¯¼å‡º ONNX
        print(f"\nğŸ“¦ å¯¼å‡º ONNX è‡³: {output_dir} (device={device}, dtype={dtype})")
        model = ORTModelForCausalLM.from_pretrained(
            weights_path,
            export=True,
            trust_remote_code=True,
            provider=provider,
            dtype=dtype if dtype in ("fp32", "fp16", "bf16") else "fp32",
        )
        model.save_pretrained(output_dir)
        tokenizer = AutoTokenizer.from_pretrained(weights_path)
        tokenizer.save_pretrained(output_dir)

        # 2. å¯é€‰ OnnxSlimï¼šå…ˆå†™ä¸´æ—¶æ–‡ä»¶ï¼ŒéªŒè¯èƒ½åŠ è½½å†è¦†ç›–ï¼Œå¤±è´¥åˆ™ä¿ç•™åŸæ–‡ä»¶
        if use_onnxslim:
            print("\nğŸª„ OnnxSlim ä¼˜åŒ–...")
            import onnxruntime as ort

            for p in Path(output_dir).glob("*.onnx"):
                orig_path = str(p)
                with tempfile.NamedTemporaryFile(suffix=".onnx", delete=False) as f:
                    tmp_path = f.name
                try:
                    slim_model = onnxslim.slim(onnx.load(orig_path))
                    onnx.save(slim_model, tmp_path)
                    # éªŒè¯ç”¨ CPU å³å¯ï¼Œé¿å…æœªè£… TensorRT æ—¶çš„ EP æŠ¥é”™
                    ort.InferenceSession(tmp_path, providers=["CPUExecutionProvider"])
                    shutil.move(tmp_path, orig_path)
                except Exception as e:
                    print(f"   âš ï¸ OnnxSlim å¤±è´¥ï¼Œä¿ç•™åŸæ–‡ä»¶: {e}")
                    if os.path.exists(tmp_path):
                        try:
                            os.unlink(tmp_path)
                        except OSError:
                            pass

        print("\nâœ¨ å¯¼å‡ºå®Œæˆã€‚")
        export_succeeded = True
    finally:
        # 3. å¯é€‰æ¸…ç†ï¼šä»…å½“å¯¼å‡ºæˆåŠŸä¸” keep_weights=False æ—¶åˆ é™¤æƒé‡ç›®å½•
        if not keep_weights and export_succeeded:
            print(f"\nğŸ—‘ï¸ æ¸…ç†æƒé‡: {weights_path}")
            try:
                shutil.rmtree(weights_path)
                print("âœ… å·²æ¸…ç†ã€‚")
            except OSError as e:
                print(f"âŒ æ¸…ç†å¤±è´¥: {e}")
        elif keep_weights:
            print(f"\nğŸ’¾ æƒé‡ä¿ç•™: {weights_path}")


def main() -> None:
    parser = argparse.ArgumentParser(description="å°† Qwen ç±»å› æœè¯­è¨€æ¨¡å‹å¯¼å‡ºä¸º ONNXã€‚")
    parser.add_argument(
        "model_id",
        nargs="?",
        default="Qwen/Qwen2.5-0.5B-Instruct",
        help="æ¨¡å‹ IDï¼Œé»˜è®¤ Qwen/Qwen2.5-0.5B-Instruct",
    )
    parser.add_argument(
        "-o",
        "--output",
        default="./qwen25_05b_onnx",
        help="ONNX è¾“å‡ºç›®å½•ï¼Œé»˜è®¤ ./qwen25_05b_onnx",
    )
    parser.add_argument(
        "-d",
        "--device",
        choices=("cpu", "cuda"),
        default="cpu",
        help="è®¾å¤‡ï¼šcpu æˆ– cudaï¼Œé»˜è®¤ cpu",
    )
    parser.add_argument(
        "-p",
        "--dtype",
        choices=("fp32", "fp16"),
        default="fp32",
        help="ç²¾åº¦ï¼šfp32ï¼ˆå…¨ç²¾åº¦ï¼‰æˆ– fp16ï¼ˆåŠç²¾åº¦ï¼‰ï¼Œé»˜è®¤ fp32",
    )
    parser.add_argument(
        "--no-keep-weights",
        action="store_true",
        help="å¯¼å‡ºæˆåŠŸååˆ é™¤ä¸‹è½½çš„æƒé‡ç›®å½•ä»¥èŠ‚çœç©ºé—´",
    )
    parser.add_argument(
        "--onnxslim",
        action="store_true",
        help="å¯¹ ONNX åš OnnxSlim ä¼˜åŒ–ï¼ˆéƒ¨åˆ†ç¯å¢ƒå¯èƒ½å¯¼è‡´åŠ è½½æ—¶ InvalidProtobufï¼‰",
    )
    args = parser.parse_args()

    export_to_onnx(
        args.model_id,
        args.output,
        device=args.device,
        dtype=args.dtype,
        keep_weights=not args.no_keep_weights,
        use_onnxslim=args.onnxslim,
    )


if __name__ == "__main__":
    main()
